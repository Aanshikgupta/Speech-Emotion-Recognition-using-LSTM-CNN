{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce257e2e",
   "metadata": {},
   "source": [
    "# Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea8858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import sklearn\n",
    "import librosa\n",
    "import warnings\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import librosa.display\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "from tqdm import notebook\n",
    "from tensorflow import keras\n",
    "import IPython.display as ipd\n",
    "import scipy.io.wavfile as wav\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn import preprocessing\n",
    "from prettytable import PrettyTable\n",
    "from matplotlib.pyplot import figure\n",
    "tf.config.run_functions_eagerly(True)\n",
    "from tensorflow.keras.layers import GRU\n",
    "from astropy.table import Table, Column\n",
    "from python_speech_features import mfcc\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, balanced_accuracy_score, classification_report\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization, LSTM, GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0489d1",
   "metadata": {},
   "source": [
    "# Step 2: Load 5 Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982ecd75",
   "metadata": {},
   "source": [
    "## Step 2.1: Load Dataset-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sample Data\n",
    "\n",
    "print(\"\\nSample Data:\")\n",
    "print(\"============\\n\")\n",
    "path = ('Datasets/SAVEE Database')\n",
    "sample_data_1 = [os.path.join(dp, f) for dp, dn, filenames in os.walk(path) \n",
    "                 for f in filenames if os.path.splitext(f)[1] == '.wav']\n",
    "\n",
    "print(\"\\nDisplaying Some Instances from Sample Data: \")\n",
    "print(\"===========================================\\n\")\n",
    "for i in range(5):\n",
    "    print(\"Audio: \")\n",
    "    ipd.display(ipd.Audio(sample_data_1[i*100]))\n",
    "    data, sampling_rate = librosa.load(sample_data_1[i*100])\n",
    "    plt.figure(figsize=(10, 2),facecolor=\"Green\")\n",
    "    librosa.display.waveplot(data, sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadf97f4",
   "metadata": {},
   "source": [
    "## Step 2.2: Load Dataset-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f06e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sample Data\n",
    "\n",
    "print(\"\\nSample Data:\")\n",
    "print(\"============\\n\")\n",
    "path = ('Datasets/EmoDB')\n",
    "sample_data_2 = [os.path.join(dp, f) for dp, dn, filenames in os.walk(path) \n",
    "                 for f in filenames if os.path.splitext(f)[1] == '.wav']\n",
    "\n",
    "print(\"\\nDisplaying Some Instances from Sample Data: \")\n",
    "print(\"===========================================\\n\")\n",
    "for i in range(5):\n",
    "    print(\"Audio: \")\n",
    "    ipd.display(ipd.Audio(sample_data_2[i*100]))\n",
    "    data, sampling_rate = librosa.load(sample_data_2[i*100])\n",
    "    plt.figure(figsize=(10, 2),facecolor=\"blue\")\n",
    "    librosa.display.waveplot(data, sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e17ec11",
   "metadata": {},
   "source": [
    "## Step 2.3: Load Dataset-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2fbde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sample Data\n",
    "\n",
    "print(\"\\nSample Data:\")\n",
    "print(\"============\\n\")\n",
    "path = ('Datasets/CREMA-D')\n",
    "sample_data_3 = [os.path.join(dp, f) for dp, dn, filenames in os.walk(path) \n",
    "                 for f in filenames if os.path.splitext(f)[1] == '.wav']\n",
    "\n",
    "print(\"\\nDisplaying Some Instances from Sample Data: \")\n",
    "print(\"===========================================\\n\")\n",
    "for i in range(5):\n",
    "    print(\"Audio: \")\n",
    "    ipd.display(ipd.Audio(sample_data_3[i*100]))\n",
    "    data, sampling_rate = librosa.load(sample_data_3[i*100])\n",
    "    plt.figure(figsize=(10, 2),facecolor=\"blue\")\n",
    "    plt.plot(data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81470b7a",
   "metadata": {},
   "source": [
    "## Step 2.4: Load Dataset-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c9ced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sample Data\n",
    "\n",
    "print(\"\\nSample Data:\")\n",
    "print(\"============\\n\")\n",
    "path = ('Datasets/TESS')\n",
    "sample_data_4 = [os.path.join(dp, f) for dp, dn, filenames in os.walk(path) \n",
    "                 for f in filenames if os.path.splitext(f)[1] == '.wav']\n",
    "\n",
    "print(\"\\nDisplaying Some Instances from Sample Data: \")\n",
    "print(\"===========================================\\n\")\n",
    "for i in range(5):\n",
    "    print(\"Audio: \")\n",
    "    ipd.display(ipd.Audio(sample_data_4[i*100]))\n",
    "    data, sampling_rate = librosa.load(sample_data_4[i*100])\n",
    "    plt.figure(figsize=(10, 2),facecolor=\"gray\")\n",
    "    librosa.display.waveplot(data, sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b5958b",
   "metadata": {},
   "source": [
    "## Step 2.5: Load Dataset-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb80a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sample Data\n",
    "\n",
    "print(\"\\nSample Data:\")\n",
    "print(\"============\\n\")\n",
    "path = ('Datasets/RAVDESS')\n",
    "sample_data_5 = [os.path.join(dp, f) for dp, dn, filenames in os.walk(path) \n",
    "                 for f in filenames if os.path.splitext(f)[1] == '.wav']\n",
    "\n",
    "print(\"\\nDisplaying Some Instances from Sample Data: \")\n",
    "print(\"===========================================\\n\")\n",
    "for i in range(5):\n",
    "    print(\"Audio: \")\n",
    "    ipd.display(ipd.Audio(sample_data_5[i*100]))\n",
    "    data, sampling_rate = librosa.load(sample_data_5[i*100])\n",
    "    plt.figure(figsize=(10, 2),facecolor=\"yellow\")\n",
    "    librosa.display.waveplot(data, sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1214ffd5",
   "metadata": {},
   "source": [
    "# Step 3: Extract MFCC Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbc470e",
   "metadata": {},
   "source": [
    "## Step 3.1: Extract MFCC Features from Dataset-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9476ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_mfcc_features_1 = pd.DataFrame()\n",
    "\n",
    "for i in notebook.tqdm(range(len(sample_data_1))):\n",
    "    data, sampling_rate = librosa.load(sample_data_1[i])\n",
    "    \n",
    "    # Get MFCC Features\n",
    "    \n",
    "    mfcc_features = mfcc(data,sampling_rate,winlen=30,nfft=661500) \n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    \n",
    "    mfcc_df = pd.DataFrame(mfcc_features)\n",
    "    \n",
    "    if(sample_data_1[i][27:-6] == 'a'):\n",
    "        mfcc_df[\"Emotion\"]=\"Anger\"\n",
    "    elif(sample_data_1[i][27:-6] == 'd'):\n",
    "        mfcc_df[\"Emotion\"]=\"Disgust\"\n",
    "    elif(sample_data_1[i][27:-6] == 'f'):\n",
    "        mfcc_df[\"Emotion\"]=\"Fear\"\n",
    "    elif(sample_data_1[i][27:-6] == 'h'):\n",
    "        mfcc_df[\"Emotion\"]=\"Happiness\"\n",
    "    elif(sample_data_1[i][27:-6] == 'n'):\n",
    "        mfcc_df[\"Emotion\"]=\"Neutral\"\n",
    "    elif(sample_data_1[i][27:-6] == 'sa'):\n",
    "        mfcc_df[\"Emotion\"]=\"Sadness\"\n",
    "    elif(sample_data_1[i][27:-6] == 'su'):\n",
    "        mfcc_df[\"Emotion\"]=\"Surprise\"\n",
    "    else:\n",
    "        mfcc_df[\"Emotion\"]=\"Other\"\n",
    "    \n",
    "    mfcc_df[\"Gender\"] = \"Male\"\n",
    "    \n",
    "    sample_data_mfcc_features_1 = sample_data_mfcc_features_1.append(mfcc_df)\n",
    "\n",
    "# Save to CSV\n",
    "sample_data_mfcc_features_1.to_csv('Audio Features/sample_data_mfcc_features_1.csv',index= False, mode='w', header=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c508753d",
   "metadata": {},
   "source": [
    "### Step 3.1.1: Print MFCC Features-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87af7960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Features\n",
    "features_1 = pd.read_csv('Audio Features/sample_data_mfcc_features_1.csv')\n",
    "features_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf515ef",
   "metadata": {},
   "source": [
    "## Step 3.2: Extract MFCC Features from Dataset-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6969f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_mfcc_features_2 = pd.DataFrame()\n",
    "\n",
    "for i in notebook.tqdm(range(len(sample_data_2))):\n",
    "    data, sampling_rate = librosa.load(sample_data_2[i])\n",
    "    \n",
    "    # Get MFCC Features\n",
    "    \n",
    "    mfcc_features = mfcc(data,sampling_rate,winlen=30,nfft=661500) \n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    \n",
    "    mfcc_df = pd.DataFrame(mfcc_features)\n",
    "    \n",
    "    if(sample_data_2[i][24:25] == 'W'):\n",
    "        mfcc_df[\"Emotion\"]=\"Anger\"\n",
    "    elif(sample_data_2[i][24:25] == 'E'):\n",
    "        mfcc_df[\"Emotion\"]=\"Disgust\"\n",
    "    elif(sample_data_2[i][24:25] == 'A'):\n",
    "        mfcc_df[\"Emotion\"]=\"Fear\"\n",
    "    elif(sample_data_2[i][24:25] == 'F'):\n",
    "        mfcc_df[\"Emotion\"]=\"Happiness\"\n",
    "    elif(sample_data_2[i][24:25] == 'N'):\n",
    "        mfcc_df[\"Emotion\"]=\"Neutral\"\n",
    "    elif(sample_data_2[i][24:25] == 'T'):\n",
    "        mfcc_df[\"Emotion\"]=\"Sadness\"\n",
    "    elif(sample_data_2[i][24:25] == 'L'):\n",
    "        mfcc_df[\"Emotion\"]=\"Boredom\"\n",
    "    else:\n",
    "        mfcc_df[\"Emotion\"]=\"Other\"\n",
    "        \n",
    "    if(sample_data_2[i][19:21] == '03' or \n",
    "       sample_data_2[i][19:21] == '10' or \n",
    "       sample_data_2[i][19:21] == '11' or\n",
    "       sample_data_2[i][19:21] == '12' or\n",
    "       sample_data_2[i][19:21] == '15' ):\n",
    "        mfcc_df[\"Gender\"]=\"Male\"\n",
    "    else:\n",
    "        mfcc_df[\"Gender\"]=\"Female\"\n",
    "    \n",
    "    \n",
    "    sample_data_mfcc_features_2 = sample_data_mfcc_features_2.append(mfcc_df)\n",
    "\n",
    "# Save to CSV\n",
    "sample_data_mfcc_features_2.to_csv('Audio Features/sample_data_mfcc_features_2.csv',index= False, mode='w', header=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a594ca",
   "metadata": {},
   "source": [
    "### Step 3.2.1: Print MFCC Features-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0d786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Features\n",
    "features_2 = pd.read_csv('Audio Features/sample_data_mfcc_features_2.csv')\n",
    "features_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06610a8f",
   "metadata": {},
   "source": [
    "## Step 3.3: Extract MFCC Features from Dataset-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def50dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_mfcc_features_3 = pd.DataFrame()\n",
    "\n",
    "female = [1002,1003,1004,1006,1007,1008,1009,1010,1012,1013,1018,1020,1021,1024,1025,1028,1029,1030,1037,1043,1046,1047,1049,\n",
    "          1052,1053,1054,1055,1056,1058,1060,1061,1063,1072,1073,1074,1075,1076,1078,1079,1082,1084,1089,1091]\n",
    "\n",
    "for i in notebook.tqdm(range(len(sample_data_3))):\n",
    "    data, sampling_rate = librosa.load(sample_data_3[i])\n",
    "    \n",
    "    # Get MFCC Features\n",
    "    \n",
    "    mfcc_features = mfcc(data,sampling_rate,winlen=30,nfft=661500) \n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    \n",
    "    mfcc_df = pd.DataFrame(mfcc_features)\n",
    "    \n",
    "    if(sample_data_3[i][26:29] == 'ANG'):\n",
    "        mfcc_df[\"Emotion\"]=\"Anger\"\n",
    "    elif(sample_data_3[i][26:29] == 'DIS'):\n",
    "        mfcc_df[\"Emotion\"]=\"Disgust\"\n",
    "    elif(sample_data_3[i][26:29] == 'FEA'):\n",
    "        mfcc_df[\"Emotion\"]=\"Fear\"\n",
    "    elif(sample_data_3[i][26:29] == 'HAP'):\n",
    "        mfcc_df[\"Emotion\"]=\"Happiness\"\n",
    "    elif(sample_data_3[i][26:29] == 'NEU'):\n",
    "        mfcc_df[\"Emotion\"]=\"Neutral\"\n",
    "    elif(sample_data_3[i][26:29] == 'SAD'):\n",
    "        mfcc_df[\"Emotion\"]=\"Sadness\"\n",
    "    else:\n",
    "        mfcc_df[\"Emotion\"]=\"Other\"\n",
    "        \n",
    "    if(int(sample_data_3[i][17:].split('_')[0]) in female):\n",
    "        mfcc_df[\"Gender\"]=\"Female\"\n",
    "    else:\n",
    "        mfcc_df[\"Gender\"]=\"Male\"\n",
    "    \n",
    "    \n",
    "    sample_data_mfcc_features_3 = sample_data_mfcc_features_3.append(mfcc_df)\n",
    "\n",
    "# Save to CSV\n",
    "sample_data_mfcc_features_3.to_csv('Audio Features/sample_data_mfcc_features_3.csv',index= False, mode='w', header=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94cd37e",
   "metadata": {},
   "source": [
    "### Step 3.3.1: Print MFCC Features-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb11fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Features\n",
    "features_3 = pd.read_csv('Audio Features/sample_data_mfcc_features_3.csv')\n",
    "features_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3d43dd",
   "metadata": {},
   "source": [
    "## Step 3.4: Extract MFCC Features from Dataset-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b7e745",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_mfcc_features_4 = pd.DataFrame()\n",
    "\n",
    "for i in notebook.tqdm(range(len(sample_data_4))):\n",
    "    data, sampling_rate = librosa.load(sample_data_4[i])\n",
    "    \n",
    "    # Get MFCC Features\n",
    "    \n",
    "    mfcc_features = mfcc(data,sampling_rate,winlen=30,nfft=661500) \n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    \n",
    "    mfcc_df = pd.DataFrame(mfcc_features)\n",
    "    \n",
    "    if(sample_data_4[i].split('_')[2][:-4] == 'angry'):\n",
    "        mfcc_df[\"Emotion\"]=\"Anger\"\n",
    "    elif(sample_data_4[i].split('_')[2][:-4] == 'disgust'):\n",
    "        mfcc_df[\"Emotion\"]=\"Disgust\"\n",
    "    elif(sample_data_4[i].split('_')[2][:-4] == 'fear'):\n",
    "        mfcc_df[\"Emotion\"]=\"Fear\"\n",
    "    elif(sample_data_4[i].split('_')[2][:-4] == 'happy'):\n",
    "        mfcc_df[\"Emotion\"]=\"Happiness\"\n",
    "    elif(sample_data_4[i].split('_')[2][:-4] == 'neutral'):\n",
    "        mfcc_df[\"Emotion\"]=\"Neutral\"\n",
    "    elif(sample_data_4[i].split('_')[2][:-4] == 'ps'):\n",
    "        mfcc_df[\"Emotion\"]=\"Surprise\"\n",
    "    elif(sample_data_4[i].split('_')[2][:-4] == 'sad'):\n",
    "        mfcc_df[\"Emotion\"]=\"Sadness\"\n",
    "    else:\n",
    "        mfcc_df[\"Emotion\"]=\"Other\"\n",
    "        \n",
    "    if(sample_data_4[i][14:].split(\"_\")[0] == 'OAF'):\n",
    "        mfcc_df[\"Gender\"]=\"Male\"\n",
    "    else:\n",
    "        mfcc_df[\"Gender\"]=\"Female\"\n",
    "    \n",
    "    \n",
    "    sample_data_mfcc_features_4 = sample_data_mfcc_features_4.append(mfcc_df)\n",
    "\n",
    "# Save to CSV\n",
    "sample_data_mfcc_features_4.to_csv('Audio Features/sample_data_mfcc_features_4.csv',index= False, mode='w', header=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0f0b48",
   "metadata": {},
   "source": [
    "### Step 3.4.1: Print MFCC Features-4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8f93d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Features\n",
    "features_4 = pd.read_csv('Audio Features/sample_data_mfcc_features_4.csv')\n",
    "features_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd318e24",
   "metadata": {},
   "source": [
    "## Step 3.5: Extract MFCC Features from Dataset-5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68634257",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_mfcc_features_5 = pd.DataFrame()\n",
    "\n",
    "for i in notebook.tqdm(range(len(sample_data_5))):\n",
    "    data, sampling_rate = librosa.load(sample_data_5[i])\n",
    "    \n",
    "    # Get MFCC Features\n",
    "    \n",
    "    mfcc_features = mfcc(data,sampling_rate,winlen=30,nfft=661500) \n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    \n",
    "    mfcc_df = pd.DataFrame(mfcc_features)\n",
    "    \n",
    "    if(sample_data_5[i][26:].split('-')[2] == '05'):\n",
    "        mfcc_df[\"Emotion\"]=\"Anger\"\n",
    "    elif(sample_data_5[i][26:].split('-')[2] == '07'):\n",
    "        mfcc_df[\"Emotion\"]=\"Disgust\"\n",
    "    elif(sample_data_5[i][26:].split('-')[2] == '06'):\n",
    "        mfcc_df[\"Emotion\"]=\"Fear\"\n",
    "    elif(sample_data_5[i][26:].split('-')[2] == '03'):\n",
    "        mfcc_df[\"Emotion\"]=\"Happiness\"\n",
    "    elif(sample_data_5[i][26:].split('-')[2] == '01'):\n",
    "        mfcc_df[\"Emotion\"]=\"Neutral\"\n",
    "    elif(sample_data_5[i][26:].split('-')[2] == '08'):\n",
    "        mfcc_df[\"Emotion\"]=\"Surprise\"\n",
    "    elif(sample_data_5[i][26:].split('-')[2] == '04'):\n",
    "        mfcc_df[\"Emotion\"]=\"Sadness\"\n",
    "    else:\n",
    "        mfcc_df[\"Emotion\"]=\"Other\"\n",
    "        \n",
    "    if(int(sample_data_5[i][26:].split('-')[6][:-4])%2 == 1):\n",
    "        mfcc_df[\"Gender\"]=\"Male\"\n",
    "    else:\n",
    "        mfcc_df[\"Gender\"]=\"Female\"\n",
    "    \n",
    "    \n",
    "    sample_data_mfcc_features_5 = sample_data_mfcc_features_5.append(mfcc_df)\n",
    "\n",
    "# Save to CSV\n",
    "sample_data_mfcc_features_5.to_csv('Audio Features/sample_data_mfcc_features_5.csv',index= False, mode='w', header=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59040f9",
   "metadata": {},
   "source": [
    "### Step 3.5.1: Print MFCC Features-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1492a1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Features\n",
    "features_5 = pd.read_csv('Audio Features/sample_data_mfcc_features_5.csv')\n",
    "features_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668269d7",
   "metadata": {},
   "source": [
    "# Step 4: Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc7faa9",
   "metadata": {},
   "source": [
    "## Step 4.1: Combine Feature of all Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c843ccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of all 5 Datasets\n",
    "frames = [features_1, features_2, features_3, features_4, features_5]\n",
    "\n",
    "# Combined Dataset \n",
    "combined = pd.concat(frames)\n",
    "combined = combined.reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "combined.to_csv('Audio Features/combined_features.csv',index= False, mode='w', header=True)\n",
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752270a7",
   "metadata": {},
   "source": [
    "## Step 4.2: Check Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e517d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "combined=pd.read_csv('Audio Features/combined_features.csv')\n",
    "df_filtered = combined[combined['Emotion'] != \"Boredom\"]\n",
    "combined = df_filtered[df_filtered['Emotion'] != \"Other\"]\n",
    "\n",
    "figure(figsize=(14, 6), dpi=80)\n",
    "plt.title('Count of Emotions', size=25)\n",
    "sns.countplot(combined['Emotion'])\n",
    "plt.ylabel('Count', size=14)\n",
    "plt.xlabel('Emotions', size=14)\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141797b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "\n",
    "figure(figsize=(7, 6), dpi=80)\n",
    "plt.title('Count of Gender', size=25)\n",
    "sns.countplot(combined['Gender'])\n",
    "plt.ylabel('Count', size=14)\n",
    "plt.xlabel('Genders', size=14)\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5a324f",
   "metadata": {},
   "source": [
    "## Step 4.3: Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028373b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Over Sampling\n",
    "\n",
    "X = combined.drop(['Emotion'], axis = 1)\n",
    "y = combined['Emotion']\n",
    "\n",
    "ros = RandomOverSampler(random_state=64)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce314e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Frequency of each Class before and After Random Over Sampling\n",
    "\n",
    "counter_1 = collections.Counter(y)\n",
    "counter_2 = collections.Counter(y_resampled)\n",
    "\n",
    "# Before Sampling\n",
    "\n",
    "plt.figure()\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(24, 5)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(counter_1.keys(), counter_1.values())\n",
    "plt.xticks([0,1,2,3,4,5,6])\n",
    "plt.title(\"Before Sampling\")\n",
    "\n",
    "# After Sampling\n",
    "\n",
    "plt.figure()\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(24, 5)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(counter_2.keys(), counter_2.values(), color = 'green')\n",
    "plt.xticks([0,1,2,3,4,5,6])\n",
    "plt.title(\"After Sampling\")\n",
    "\n",
    "plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.5,wspace=0.35)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f420f959",
   "metadata": {},
   "source": [
    "## Step 4.4: Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29a4d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "\n",
    "y1 = y_resampled\n",
    "y2 = X_resampled[\"Gender\"]\n",
    "\n",
    "X = X_resampled.drop([\"Gender\"],axis=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(y1.shape)\n",
    "print(y2.shape)\n",
    "print(X_scaled.shape)\n",
    "\n",
    "print(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bbbb0c",
   "metadata": {},
   "source": [
    "## Step 4.5: Label Encoding Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be347f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions\n",
    "encoder_1 = preprocessing.LabelEncoder()\n",
    "y1_encoded = encoder_1.fit_transform(y1)\n",
    "\n",
    "# Gender\n",
    "encoder_2 = preprocessing.LabelEncoder()\n",
    "y2_encoded = encoder_2.fit_transform(y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86342dc5",
   "metadata": {},
   "source": [
    "# Step 5: Split Dataset in Traing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f64608",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "i=0\n",
    "while i<len(y1_encoded):\n",
    "    label=[]\n",
    "    label.append([y1_encoded[i]])\n",
    "    label.append([y2_encoded[i]])\n",
    "    labels.append(label)\n",
    "    i+=1\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643121f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test= train_test_split(X_scaled, labels, test_size=0.1, random_state = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdca31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = [Y_train[:,1],Y_train[:,0]]\n",
    "Y_test = [Y_test[:,1],Y_test[:,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b69b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train\n",
    "X_test = X_test\n",
    "y1_train = Y_train[0]\n",
    "y1_test = Y_test[0]\n",
    "y2_train = Y_train[1]\n",
    "y2_test = Y_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11acc826",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y1_train.shape)\n",
    "print(y1_test.shape)\n",
    "print(y2_train.shape)\n",
    "print(y2_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31398940",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y1_train)\n",
    "df[\"1\"] = y2_train\n",
    "y_train = np.array(df)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c94b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y1_test)\n",
    "df[\"1\"] = y2_test\n",
    "y_test = np.array(df)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042a48d0",
   "metadata": {},
   "source": [
    "# Step 6: Train CNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b165cd58",
   "metadata": {},
   "source": [
    "## Step 6.1: For Gender Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10944194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_test, y_train, y_test = X_train, X_test, y1_train, y1_test\n",
    "\n",
    "size = 13\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test) \n",
    "y_test = np.array(y_test)\n",
    "x_train = x_train.reshape(x_train.shape[0], size, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], size, 1)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Create Model\n",
    "\n",
    "model_1_g=Sequential()\n",
    "model_1_g.add(Conv1D(512, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\n",
    "model_1_g.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "model_1_g.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model_1_g.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "model_1_g.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model_1_g.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "model_1_g.add(Dropout(0.2))\n",
    "\n",
    "model_1_g.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model_1_g.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "model_1_g.add(GlobalAveragePooling1D())\n",
    "\n",
    "model_1_g.add(Flatten())\n",
    "model_1_g.add(Dense(units=32, activation='relu'))\n",
    "model_1_g.add(Dropout(0.3))\n",
    "\n",
    "model_1_g.add(Dense(units=2, activation='softmax'))\n",
    "model_1_g.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "\n",
    "model_1_g.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c5cd7f",
   "metadata": {},
   "source": [
    "### Step 6.1.1: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd63c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000001)\n",
    "history_1_g = model_1_g.fit(x_train, y_train, batch_size=64, epochs=250, validation_data=(x_test, y_test), callbacks=[rlrp])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed44942a",
   "metadata": {},
   "source": [
    "### Step 6.1.2: Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff19ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "epochs = [i for i in range(250)]\n",
    "fig , ax = plt.subplots(1,2)\n",
    "train_acc = history_1_g.history['accuracy']\n",
    "train_loss = history_1_g.history['loss']\n",
    "test_acc = history_1_g.history['val_accuracy']\n",
    "test_loss = history_1_g.history['val_loss']\n",
    "\n",
    "fig.set_size_inches(20,6)\n",
    "ax[0].plot(epochs , train_loss , label = 'Training Loss')\n",
    "ax[0].plot(epochs , test_loss , label = 'Testing Loss')\n",
    "ax[0].set_title('Training & Testing Loss')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "\n",
    "ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n",
    "ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\n",
    "ax[1].set_title('Training & Testing Accuracy')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97978c0",
   "metadata": {},
   "source": [
    "### Step 6.1.3: Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f20550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "\n",
    "model_1_g.save('Trained Models/model_1_g.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01337af2",
   "metadata": {},
   "source": [
    "## Step 6.2: For Emotion Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4607388",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = X_train, X_test, y2_train, y2_test\n",
    "\n",
    "size = 13\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test) \n",
    "y_test = np.array(y_test)\n",
    "x_train = x_train.reshape(x_train.shape[0], size, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], size, 1)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Create Model\n",
    "\n",
    "model_1_e=Sequential()\n",
    "model_1_e.add(Conv1D(512, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\n",
    "model_1_e.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "model_1_e.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model_1_e.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "model_1_e.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model_1_e.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "model_1_e.add(Dropout(0.2))\n",
    "\n",
    "model_1_e.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model_1_e.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "model_1_e.add(GlobalAveragePooling1D())\n",
    "\n",
    "model_1_e.add(Flatten())\n",
    "model_1_e.add(Dense(units=32, activation='relu'))\n",
    "model_1_e.add(Dropout(0.3))\n",
    "\n",
    "model_1_e.add(Dense(units=7, activation='softmax'))\n",
    "model_1_e.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "\n",
    "model_1_e.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a30b7a1",
   "metadata": {},
   "source": [
    "### Step 6.2.1: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91bc7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000001)\n",
    "history_1_e = model_1_e.fit(x_train, y_train, batch_size=64, epochs=250, validation_data=(x_test, y_test), callbacks=[rlrp])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef54f23",
   "metadata": {},
   "source": [
    "### Step 6.2.2: Learning Curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ce7e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "epochs = [i for i in range(250)]\n",
    "fig , ax = plt.subplots(1,2)\n",
    "train_acc = history_1_e.history['accuracy']\n",
    "train_loss = history_1_e.history['loss']\n",
    "test_acc = history_1_e.history['val_accuracy']\n",
    "test_loss = history_1_e.history['val_loss']\n",
    "\n",
    "fig.set_size_inches(20,6)\n",
    "ax[0].plot(epochs , train_loss , label = 'Training Loss')\n",
    "ax[0].plot(epochs , test_loss , label = 'Testing Loss')\n",
    "ax[0].set_title('Training & Testing Loss')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "\n",
    "ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n",
    "ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\n",
    "ax[1].set_title('Training & Testing Accuracy')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf95aea8",
   "metadata": {},
   "source": [
    "### Step 6.2.3: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf95d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "\n",
    "model_1_e.save('Trained Models/model_1_e.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c257c8",
   "metadata": {},
   "source": [
    "# Step 7: Train LSTM Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fb8155",
   "metadata": {},
   "source": [
    "## Step 7.1: For Gender Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb401557",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = X_train, X_test, y1_train, y1_test\n",
    "\n",
    "size = 13\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test) \n",
    "y_test = np.array(y_test)\n",
    "x_train = x_train.reshape(x_train.shape[0], size, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], size, 1)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8142c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Embedding, Dropout, GlobalAveragePooling1D, Flatten, SpatialDropout1D, Bidirectional\n",
    "\n",
    "lstm_1_g = Sequential()\n",
    "lstm_1_g.add(Dense(256,  activation='relu', input_shape=(x_train.shape[1],1)))\n",
    "lstm_1_g.add(Dense(128, activation='relu'))\n",
    "lstm_1_g.add(Dense(64, activation='relu'))\n",
    "lstm_1_g.add(Dense(32, activation='relu'))\n",
    "lstm_1_g.add(Dense(16, activation='relu'))\n",
    "lstm_1_g.add(Dense(8, activation='relu'))\n",
    "lstm_1_g.add(Flatten())\n",
    "lstm_1_g.add(Dense(2, activation='softmax'))\n",
    "lstm_1_g.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "lstm_1_g.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e81095",
   "metadata": {},
   "source": [
    "### Step 7.1.1: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6469c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_2_g = lstm_1_g.fit(x_train, y_train, epochs=250, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4580c25b",
   "metadata": {},
   "source": [
    "### Step 7.1.2: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bcf709",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "epochs = [i for i in range(250)]\n",
    "fig , ax = plt.subplots(1,2)\n",
    "train_acc = history_2_g.history['accuracy']\n",
    "train_loss = history_2_g.history['loss']\n",
    "test_acc = history_2_g.history['val_accuracy']\n",
    "test_loss = history_2_g.history['val_loss']\n",
    "\n",
    "fig.set_size_inches(20,6)\n",
    "ax[0].plot(epochs , train_loss , label = 'Training Loss')\n",
    "ax[0].plot(epochs , test_loss , label = 'Testing Loss')\n",
    "ax[0].set_title('Training & Testing Loss')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "\n",
    "ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n",
    "ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\n",
    "ax[1].set_title('Training & Testing Accuracy')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ef8ebc",
   "metadata": {},
   "source": [
    "### Step 7.1.3: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32be4a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "\n",
    "lstm_1_g.save('Trained Models/lstm_1_g.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633e7cc0",
   "metadata": {},
   "source": [
    "## Step 7.2: For Emotion Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f48238",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = X_train, X_test, y2_train, y2_test\n",
    "\n",
    "size = 13\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test) \n",
    "y_test = np.array(y_test)\n",
    "x_train = x_train.reshape(x_train.shape[0], size, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], size, 1)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Create Model\n",
    "\n",
    "lstm_1_e = Sequential()\n",
    "lstm_1_e.add(Dense(256,  activation='relu', input_shape=(x_train.shape[1],1)))\n",
    "lstm_1_e.add(Dense(128, activation='relu'))\n",
    "lstm_1_e.add(Dense(64, activation='relu'))\n",
    "lstm_1_e.add(Dense(32, activation='relu'))\n",
    "lstm_1_e.add(Flatten())\n",
    "lstm_1_e.add(Dense(16, activation='relu'))\n",
    "lstm_1_e.add(Dense(10, activation='relu'))\n",
    "lstm_1_e.add(Dense(8, activation='relu'))\n",
    "lstm_1_e.add(Flatten())\n",
    "lstm_1_e.add(Dense(7, activation='softmax'))\n",
    "lstm_1_e.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "lstm_1_e.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14b0d11",
   "metadata": {},
   "source": [
    "### Step 7.2.1: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a52c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_2_e = lstm_1_e.fit(x_train, y_train, epochs=250, batch_size=16, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c250cbd",
   "metadata": {},
   "source": [
    "### Step 7.2.2: Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c8d319",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "epochs = [i for i in range(250)]\n",
    "fig , ax = plt.subplots(1,2)\n",
    "train_acc = history_2_e.history['accuracy']\n",
    "train_loss = history_2_e.history['loss']\n",
    "test_acc = history_2_e.history['val_accuracy']\n",
    "test_loss = history_2_e.history['val_loss']\n",
    "\n",
    "fig.set_size_inches(20,6)\n",
    "ax[0].plot(epochs , train_loss , label = 'Training Loss')\n",
    "ax[0].plot(epochs , test_loss , label = 'Testing Loss')\n",
    "ax[0].set_title('Training & Testing Loss')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "\n",
    "ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n",
    "ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\n",
    "ax[1].set_title('Training & Testing Accuracy')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de595d5e",
   "metadata": {},
   "source": [
    "### Step 7.2.3: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d246754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "\n",
    "lstm_1_g.save('Trained Models/lstm_1_e.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427af137",
   "metadata": {},
   "source": [
    "# Step 8: Comparision of CNN and LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbc622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPerformance with Models\")\n",
    "print(\"=======================\\n\")\n",
    "x=PrettyTable()\n",
    "x.add_column(\"Classifier Name\",[\"CNN (Gender)\",\"CNN (Emotion)\",\"LSTM (Gender)\", \"LSTM (Emotion)\"])\n",
    "x.add_column(\"Accuracy Score\",[history_1_g.history['accuracy'][-1],history_1_e.history['accuracy'][-1],history_2_g.history['accuracy'][-1],history_2_e.history['accuracy'][-1]])\n",
    "print(x)\n",
    "\n",
    "x = [\"CNN (Gender)\",\"CNN (Emotion)\",\"LSTM (Gender)\", \"LSTM (Emotion)\"]\n",
    "y = [round(history_1_g.history['accuracy'][-1],2),round(history_1_e.history['accuracy'][-1],2),round(history_2_g.history['accuracy'][-1],2),round(history_2_e.history['accuracy'][-1],2)]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (14,5)\n",
    "fig, ax = plt.subplots() \n",
    "width = 0.25\n",
    "ind = np.arange(len(y))\n",
    "ax.barh(ind, y, width, color=\"blue\")\n",
    "ax.set_yticks((ind+width/2)-0.1)\n",
    "ax.set_yticklabels(x, minor=False)\n",
    "for i, v in enumerate(y):\n",
    "    ax.text(v + 0.01, i , str(v), color='blue', fontweight='bold')\n",
    "plt.title('Comparision')\n",
    "plt.xlabel('Accuracy Score')\n",
    "plt.ylabel('Classifiers')      \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
